\section*{Chapter 3. Stochastic Processes: general theory}
\subsection*{3.1: Definition, distribution, and versions}
\begin{definition*}
Given $(\Omega,\F,\mathbf P)$, a \emph{stochastic process} (S.P.) $\{X_t\}$ is a collection of R.V.-s indexed by $t\in I$. We call $t \mapsto X_t(\omega)$ the \emph{sample path} of the S.P.
\end{definition*}

\begin{definition*}
A \emph{random walk} is the sequence $S_n = \sum_{i=1}^n \xi_i$, where $\xi_i$ are i.i.d. real-valued R.V.-s defined on the same $(\Omega,\F,\mathbf P)$. When $\xi_i \in \mathbb Z$ we say it's a random walk on the integers, and we call $\xi_i\in\{-1,1\}$ a simple random walk.
\end{definition*}

% do we care about this??
\begin{theorem*}
Consider the random walk $S_n$ when $\E\xi_i=0$ and $\E\xi_i^2=1$. Take the linear interpolation of $S_n$, scale space by $n^{-1/2}$ and time by $n^{-1}$. Taking $n\to\infty$ we get what we call \emph{Brownian motion} on $0\leq t \leq 1$.
\end{theorem*}

\begin{definition*} Given $N < \infty$ and $t_1,\dots,t_N\in \mathcal I$, we denote the (joint) distribution of $(X_{t_1},\dots,X_{t_N})$ by $F_{t_1,\dots,t_N}(\cdot)$, i.e. $F_{t_1,\dots,t_N}(\alpha_1,\dots,\alpha_N) = \mathbf P(X_{t_1 \leq \alpha_1},\dots,X_{t_N\leq\alpha_N})$ for all $\alpha_i\in\mathbb R$. We call the collection of functions $F_{t_1,\dots,t_N}(\cdot)$ the finite dimensional distributions (f.d.d.) of the S.P.
\end{definition*}

\begin{definition*}
With $\G_t$ the smallest $\sigma$-field containing $\sigma(X_s)$ for any $0\leq s\leq t$, we say that a S.P. $\{X_t\}$ has \emph{indepedent increments} if $X_{t+h}-X_t$ is independent of $\G_t$ for any $h>0$ and all $t \geq 0$. This property is determined by the f.d.d. That is, if $X_{t_1}, X_{t_2}-X_{t_1},\dots,X_{t_n} - X_{t_{n-1}}$ are mutually independent for all $n < \infty$ and $0 \leq t_1 < t_2 < \dots < t_n < \infty$ then the S.P. $\{X_t\}$ has independent increments.
\end{definition*}

\begin{remark*}
For example, both the random walk and the Brownian motion have indepedent increments.
\end{remark*}

\begin{example*}
Consider $\Omega = [0,1]$ with Borel $\sigma$-field and Uniform law. Define $Y_t(\omega) \equiv 0$ and $X_t(\omega) = \1_{\{\omega\}}(t)$. Note that $\mathbf P(X_t=Y_t)=1$ for any fixed $t$ but $\mathbf P(X_t=Y_t \, \forall t\in[0,1])=0$ and no sample paths $t \mapsto X_t(\omega)$ are continuous.
\end{example*}

\begin{definition*}
Two S.P. $\{X_t\}$ and $\{Y_t\}$ are called \emph{versions} of one another if they have the same f.d.d.s.
\end{definition*}

\begin{definition*}
A S.P. $\{Y_t\}$ is called a \emph{modification} of another S.P. $\{X_t\}$ if $\mathbf P(X_t = Y_t) = 1$ for all $t$.
\end{definition*}

\begin{exercise*}
If $\{Y_t\}$ is a modification of $\{X_t\}$ then $\{Y_t\}$ is also a version of $\{X_t\}$.
\end{exercise*}

\begin{example*}
Consider $\Omega = \{HH,TT,HT,TH\}$ with uniform probability measure, corresponding to two indepdent fair coin tosses with outcome $\omega=(\omega_1,\omega_2)$. Let $X_t(\omega) = \1_{[0,1)}(t)I_H(\omega_1) + \1_{[1,2)}(t)I_H(\omega_2)$ for $0\leq t<2$, and let $Y_t(\omega) = 1-X_t(\omega)$. These are version of each other but not modifications of each other.
\end{example*}

\begin{definition*}
We say that a collection of f.d.d.s is \emph{consistent} if 
\[
	\lim_{\alpha_k \uparrow \infty} F_{t_1,\dots,t_N}(\alpha_1,\dots,\alpha_N) = F_{t_1,\dots,t_{k-1},t_{k+1},\dots,t_N} (\alpha_1,\dots,\alpha_{k-1},\alpha_{k+1},\dots,\alpha_N),
\]
for any $1\leq k\leq N$, $t_1<t_2<\dots<t_N$ and $\alpha_i\in\mathbb R$.
\end{definition*}

\begin{proposition*}
The f.d.d.s of any S.P. must be consistent. Conversely, for any consistent collection f.d.d.s, there exists a probability space $(\Omega, \F,\mathbf P)$ and a stochastic process $\{X_t(\omega)\}$ on it, whose f.d.d.s are in agreement with the given collection. Further, the restriction of $\mathbf P$ to the $\sigma$-field $\F_X$ is uniquely determined by the given collection of f.d.d.
\end{proposition*}

%excluding the rest of 3.1 because don't have to know about cylindrical \sigma-field stuff

\subsection*{3.2: Characteristic functions, Gaussian variables and processes}
\begin{definition*}
A \emph{random vector} $\uX = (X_1,\dots,X_n)$ with values in $\mathbb R^n$ has the \emph{characteristic function} 
$\Phi_{\uX}(\utheta) = \E[\exp{(i \sum_{k=1}^n \theta_k X_k)}]$, 
where $\utheta = (\theta_1,\dots,\theta_n) \in \mathbb R^n$.
\end{definition*}

\begin{remark*}
The characteristic function exists for any $\uX$ because trig functions are bounded.
\end{remark*}

\begin{proposition*}
The characteristic function determines the law of a random vector. That is, if $\Phi_\uX(\utheta) = \Phi_{\underline{Y}}(\utheta)$ for all $\utheta$ then $\uX$ has the same law (= probability measure on $\mathbb R^n$) as $\underline Y$.
\end{proposition*}

\begin{exercise*}
If $X_n \overset{\mathcal L}{\to} X$ then $\Phi_{X_n}(\theta) \to \Phi_X(\theta)$ for any $\theta$.
\end{exercise*}

\begin{remark*}
The converse of the previous exercise is also true.
\end{remark*}

\begin{definition*}
We say that a random vector $\uX = (X_1,\dots,X_n)$ has a \emph{probability density function} $f_\uX$ if 
\[
	\mathbf P(a_i \leq X_i \leq b_i,i=1,\dots,n) = \int_{a_1}^{b_1} \cdots \int_{a_n}^{b_n} f_\uX(x_1,\dots,x_n)\,dx_n\,\cdots\,dx_1
\]
for every $a_i<b_i,i=1,\dots,n$. Such density $f_\uX$ must be a non-negative Borel measurable function with $\int_{\mathbb R^n} f_\uX(\underline x) \, d\underline x = 1$. $f_\uX$ can be called the \emph{joint density} of $X_i$.
\end{definition*}

\begin{proposition*}
If $\uX = (X_1,\dots,X_n)$, with $X_i$ R.V.s, then $X_i$ are mutually independent if and only if $\Phi_\uX(\utheta) = \E \left[ \prod_{k=1}^n e^{i\theta_kX_k}\right] = \prod_{k=1}^n \Phi_{X_k}(\theta_k)$ for all $\utheta \in \mathbb R^n$.
\end{proposition*}

%skipping the note about positive semidefinite

\begin{definition*}
We say that a \emph{random vector} $\uX=(X_1,\dots,X_n)$ has a \emph{Gaussian} (or multivariate Normal) distribution if $\Phi_\uX(\utheta) = \exp{(-\frac12 \langle \utheta, \Sigma\utheta \rangle + i \langle \utheta, \underline \mu \rangle)}$ for some positive semidefinite $n\times n$ matrix $\Sigma$, some $\underline \mu \in \mathbb R^n$, and all $\utheta \in \mathbb R^n$.
\end{definition*}

\begin{remark*}
When $n=1$ we say that a R.V. $X$ is Gaussian if $\E[e^{i\theta X}] = e^{-\frac12 \theta^2 \sigma^2 + i\theta\mu}$.
\end{remark*}

\begin{definition*}
We say that $\uX$ has a \emph{non-degenerate} Gaussian distribution if $\Sigma$ is invertible.
\end{definition*}

\begin{proposition*}
A random vector $\uX$ with a non-degenerate Gaussian distribution has the density
$f_\uX(\underline x) = (2\pi)^{-n/2}(\det\Sigma)^{-1/2} \exp{(-\frac12 \langle\underline x - \underline \mu, \Sigma^{-1}(\underline x - \underline \mu)\rangle)}$. In particular, if $\sigma^2 > 0$, then a Gaussian R.V. $X$ has density $f_X(x) = (2\pi)^{-1/2} \sigma^{-1} \exp{(-\frac12 (x-\mu)^2/\sigma^2)}$.
\end{proposition*}

\begin{proposition*}
The \emph{parameters of the Gaussian distribution} are $\mu_j = \E X_j$ and $\Sigma_{jk} = \E[(X_j-\mu_j)(X_k-\mu_k)]$.
\end{proposition*}

% skipped the exercises but they might be important?

\begin{proposition*}
If a Gaussian random vector $\uX = (X_1,\dots,X_n)$ has uncorrelated coordinates, then its coordinates are also mutually independent.
\end{proposition*}

\begin{proposition*}
Suppose a sequence of $n$-dimensional Gaussian random vectors $\uX^{(k)}$ converges in 2-mean to an $n$-dimensional random vector $\uX$, that is, $\E[(X_i - X_i^{(k)})^2] \to 0$ as $k\to\infty$ for each $i$. Then, $\uX$ is a Gaussian random vector, whose parameters are the limits of the corresponding parameters of $\uX^{(k)}$.
\end{proposition*}

\begin{proposition*}
A random vector $\uX$ has the Gaussian distribution if and only if $(\sum_{i=1}^n a_{ji}X_i, j=1,\dots,m)$ is a Gaussian random vector for any non-random coefficients $a_{ij} \in \mathbb R$.
\end{proposition*}

\begin{definition*}
A stochastic process (S.P.) $\{X_t\}$ is \emph{Gaussian} if for all $n < \infty$ and all $t_1,\dots,t_n \in \mathcal I$ the random vector $(X_{t_1},\dots,X_{t_n})$ has a Gaussian distribution, that is, all f.d.d.s of the process are Gaussian
\end{definition*}

\begin{corollary*}
All distributional properties of Gaussian processes are determined by the mean $\mu(t) = \E X_t$ of the process and its auto-covariance function $\rho(t,s) = \E[(X_t-\mu(t))(X_s-\mu(s))]$.
\end{corollary*}

\begin{proposition*}
If $\text{Cov}(Y_{t+h} - Y_t,Y_s) = 0$ for a Gaussian stochastic process $\{Y_t\}$, all $t\geq s$ and $h>0$, then the S.P. $\{Y_t\}$ has uncorrelated hence independent increments (which is thus also equivalent to $\E(Y_{t+h} - Y_t | \sigma(Y_s, s\leq t)) = \E(Y_{t+h}-Y_t)$ for any $t \geq 0$ and $h > 0$).
\end{proposition*}

\begin{proposition*}
If the S.P. $\{X_t, t\in\mathcal I\}$ and the Gaussian S.P. $\{X_t^{(k)},t\in\mathcal I\}$ are such that $\E[(X_t - X_t^{(k)})^2]\to0$ as $k\to\infty$, for each fixed $t \in \mathcal I$, then $X_t$ is a Gaussian S.P. with mean and auto-covariance functions that are the pointwise limits of those for $X_t^{(k)}$.
\end{proposition*}

%some examples/exercises here

\begin{definition*}
A stochastic process $\{X_t\}$ indexed by $t\in\mathbb R$ is called \emph{(strong sense) stationary} if its f.d.d. satisfy
$F_{t_1,\dots,t_N}(\alpha_1,\dots,\alpha_N) = F_{t_1+\tau,\dots,t_N+\tau}(\alpha_1,\dots,\alpha_N)$ for all $\tau\in\mathbb R$, $N<\infty$, $\alpha_i\in\mathbb R$, and $t_1<\dots<t_N$. A similar definition applies to discrete time S.P. indexed by $t$ on the integers, just then $t_i$ and $\tau$ take only integer values.
\end{definition*}

\begin{proposition*}
A Gaussian S.P. is stationary if and only if $\mu(t) = \mu$ (a constant) and $\rho(t,s) = r(\abs{t-s})$, where $r:\mathbb R \to \mathbb R$ is a function of the time difference. (A stochastic process whose mean and auto-covariance function satisfy these two properties is called \emph{weak sense (or covariance) stationary}. In general, a weak sense stationary process is not a strong sense stationary process. However, for Gaussians, they are the same.
\end{proposition*}

% interesting exercise to include?

\begin{definition*}
A process $\{X_t, t\geq0\}$ has \emph{stationary increments} if $X_{t+h}-X_t$ and $X_{s+h}-X_s$ have the same law for all $s,t,h\geq0$. The same definition applies to discrete time S.P. except for integer values.
\end{definition*}

\begin{example*}
A sequence of i.i.d. r.v.s $\{X_n,n\in\mathbb Z\}$ is a discrete time stationary process. However, many processes are not stationary. For example, the random walk $S_n = \sum_{i=1}^n X_i$ is a non-stationary S.P. when $\E X_1 = 0$ and $\E X_1^2 = 1$. Otherwise the law of $S_n$ and in particular its second moment would not depend on $n$, but clearly $\E S_n = n$. Also, every stationary process has stationary increments, but the random walk $S_n$ has stationary increments but is not stationary.
\end{example*}

\subsection*{3.3: Sample path continuity}
\begin{definition*}
$\{X_t\}$ has \emph{continuous sample path} with probability 1 if $\mathbf P(\{\omega : t\mapsto X_t(\omega) \text{ is continuous}\})=1$. Similarly, we use the term \emph{continuous modification} to denote a modification $\{\tilde X_t\}$ of $\{X_t\}$ that has continous sample path with probability 1.
\end{definition*}

%maybe inline this?
\begin{definition*}
A S.P. $Y_t$ is \emph{locally H\"older continuous} with exponent $\gamma$ if for some $c < \infty$ and a R.V. $h(\omega)>0$, 
\[
	\mathbf P\left( \left\{ \omega : \sup_{0\leq s,t\leq T, \abs{t-s} \leq h(\omega)} \abs{Y_t(\omega) - Y_s(\omega)} \leq c \abs{t-s}^\gamma \right\}\right) = 1.
\]
Note the ``locally'' refers to $h(\omega)$; if it holds for unrestricted $t,s$ we say $Y_t$ is \emph{globally/uniformly H\"older continuous} with exponent $\gamma$.
\end{definition*}

\begin{theorem*}[Kolmogorov's continuity theorem]
Given a S.P. $\{X_t, t\in[0,T]\}$, suppose there exist $\alpha,\beta,c,h_0>0$ so that 
$\E(\abs{X_{t+h} - X_t}^\alpha) \leq ch^{1+\beta}$ for all $0 \leq t,t+h \leq T$, $0 < h < h_0$. Then, there exists a continuous modification $Y_t$ of $X_t$ so that $Y_t$ is also locally H\"older continuous with exponent $\gamma$ for any $0 < \gamma < \beta/\alpha$.
\end{theorem*}

\begin{example*}
Consider the S.P. $X_t(\omega) = I_{\{U\leq t\}}(\omega)$ for $t\in[0,1]$ where $U$ is Uniform on $[0,1]$ (jumps up at a random point). Note that $\abs{X_{t+h}-X_t} = 1$ if $0\leq t<U\leq t+h$ and equals zero otherwise. So, $\E(\abs{X_{t+h}-X_t}^\alpha) = U((t,t+h]) \leq h$ for any $h>0$ and $t\geq0$. However, $X_t(\omega)$ is discontinuous as $t=U(\omega)$ when $\omega\not=0$. So $\{X_t\}$ a.s. has discontinuous sample paths (and it can be shown it has no continuous modification).
\end{example*}

%exercise that we had as homework; left out some
\begin{exercise*}
Suppose the S.P. $X_t$ has zero mean and variance one always. Then $\abs{\E(X_t X_{t+h})} \leq 1$ for any $h > 0$. Also, if we have for $p>1$ $\E(X_t X_{t+h}) \geq 1 - \lambda h^p$ for all $0 < h \leq h_0$, then by Kolmogorov, $X_t$ has a continuous modification. Now suppose $X_t$ a Gaussian S.P. with $\E X_t=0$ and $E X^2_t = 0$. Then if $X_t$ satisfies $\E(X_tX_{t+h})\geq1-\lambda h^p$ for all $0<h\leq h_0$, for some finite $\lambda$, $p > 0$ (not $p > 1$), then it has a modification which is locally H\"older continuous for all $\gamma < p/2$.
\end{exercise*}

\begin{example*}[Random telegraph noise]
Let $\tau_i, i=\in\mathbb N$ be independent random times each having the Exponential(1) distribution, i.e. $\mathbf P(\tau_i \leq x) = 1 - e^{-x}$ for all $i$, $x\geq0$. Starting at $R_0\in\{-1,1\}$ such that $\mathbf P(R_0=1)=1/2$, the S.P. $R_t$ alternately jumps between $\pm 1$ at the random times $s_k = \sum_{i=1}^k \tau_i$, so $R_t$ is constant on each $(s_k,s_{k+1})$. Since almost surely $s_1<\infty$ this S.P. does not have a continuous modification. However $\E(R_tR_{t+\varepsilon}) = \mathbf P(R_t = R_{t+\varepsilon}) - \mathbf P(R_t \not= R_{t+\varepsilon}) = 1 - 2\mathbf P(R_t \not= R_{t+\varepsilon})$, and for any $t\geq 0$ and $\varepsilon >0$, $\varepsilon^{-1} \mathbf P(R_t \not= R_{t+\varepsilon}) \leq \varepsilon^{-1} \mathbf P(\tau_i \leq \varepsilon) = \varepsilon^{-1}(1-e^{-\varepsilon}) \leq 1$, so it satisfies $\E(R_t R_{t+\varepsilon}) \geq 1 - \lambda h^p$ for $p=1$ and $\lambda =2$ and therefore has a continuous modification.
\end{example*}

\begin{definition*}
An S.P. $X_t$ has \emph{right-continuous with left limits} (RCLL) sample paht if for a.e. $\omega$, the path $X_t(\omega)$ is RCLL at any $t$, i.e. for $h \downarrow 0$ both $X_{t+h}(\omega) \to X_t(\omega)$ and the limit of $X_{t-h}(\omega)$ exists). Similarly, a modification having RCLL sample path with probability one is called an \emph{RCLL modification} of the S.P.
\end{definition*}

\begin{definition*}
Two S.P.s $X_t$ and $Y_t$ have \emph{indistinguishable} sample path if $\mathbf P(X_t = Y_t \text{ for all } t) = 1$.
\end{definition*}

\begin{exercise*}
If $X_n, Y_n$ are discrete time S.P.s that are modifications of each other then they have indistinguishable sample path. If $X_t,Y_t$ are continuous time S.P.s that are modifications of each other, and both have RCLL sample paths a.s., then they are indistinguishable. However, if you consider $Y_t(\omega) \equiv 0$ and $X_t(\omega) = I_\{\omega\}(t)$ (where $\omega \in [0,1]$ uniformly) then $X_t,Y_t$ are \emph{not} indistinguishable.
\end{exercise*}

\begin{proposition*}
For sample paths of any S.P.: H\"older continuity $\implies$ Continuous with probability one $\implies$ RCLL.
\end{proposition*}

\begin{theorem*}[Fubini's theorem]
If $X_t$ has RCLL sample path and for some interval $I$ and $\sigma$-field $\mathcal H$, almost surely $\int_I \E[\abs{X_t}|\mathcal H] \, dt$ is finite then almost surely $\int_I X_t\,dt$ is finite and $\int_I \E[X_t | \mathcal H]\,dt = \E \left[ \int_I X_t \, dt | \mathcal H \right]$.
\end{theorem*}