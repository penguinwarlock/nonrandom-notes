\section*{Chapter 3. Stochastic Processes: general theory}
\subsection*{3.1: Definition, distribution, and versions}
\begin{definition*}
Given $(\Omega,\F,\mathbf P)$, a \emph{stochastic process} (S.P.) $\{X_t\}$ is a collection of R.V.-s indexed by $t\in I$. We call $t \mapsto X_t(\omega)$ the \emph{sample path} of the S.P.
\end{definition*}

\begin{definition*}
A \emph{random walk} is the sequence $S_n = \sum_{i=1}^n \xi_i$, where $\xi_i$ are i.i.d. real-valued R.V.-s defined on the same $(\Omega,\F,\mathbf P)$. When $\xi_i \in \mathbb Z$ we say it's a random walk on the integers, and we call $\xi_i\in\{-1,1\}$ a simple random walk.
\end{definition*}

% do we care about this??
\begin{theorem*}
Consider the random walk $S_n$ when $\E\xi_i=0$ and $\E\xi_i^2=1$. Take the linear interpolation of $S_n$, scale space by $n^{-1/2}$ and time by $n^{-1}$. Taking $n\to\infty$ we get what we call \emph{Brownian motion} on $0\leq t \leq 1$.
\end{theorem*}

\begin{definition*} Given $N < \infty$ and $t_1,\dots,t_N\in \mathcal I$, we denote the (joint) distribution of $(X_{t_1},\dots,X_{t_N})$ by $F_{t_1,\dots,t_N}(\cdot)$, i.e. $F_{t_1,\dots,t_N}(\alpha_1,\dots,\alpha_N) = \mathbf P(X_{t_1 \leq \alpha_1},\dots,X_{t_N\leq\alpha_N})$ for all $\alpha_i\in\mathbb R$. We call the collection of functions $F_{t_1,\dots,t_N}(\cdot)$ the finite dimensional distributions (f.d.d.) of the S.P.
\end{definition*}

\begin{definition*}
With $\G_t$ the smallest $\sigma$-field containing $\sigma(X_s)$ for any $0\leq s\leq t$, we say that a S.P. $\{X_t\}$ has \emph{indepedent increments} if $X_{t+h}-X_t$ is independent of $\G_t$ for any $h>0$ and all $t \geq 0$. This property is determined by the f.d.d. That is, if $X_{t_1}, X_{t_2}-X_{t_1},\dots,X_{t_n} - X_{t_{n-1}}$ are mutually independent for all $n < \infty$ and $0 \leq t_1 < t_2 < \dots < t_n < \infty$ then the S.P. $\{X_t\}$ has independent increments.
\end{definition*}

\begin{remark*}
For example, both the random walk and the Brownian motion have indepedent increments.
\end{remark*}

\begin{example*}
Consider $\Omega = [0,1]$ with Borel $\sigma$-field and Uniform law. Define $Y_t(\omega) \equiv 0$ and $X_t(\omega) = \1_{\{\omega\}}(t)$. Note that $\mathbf P(X_t=Y_t)=1$ for any fixed $t$ but $\mathbf P(X_t=Y_t \, \forall t\in[0,1])=0$ and no sample paths $t \mapsto X_t(\omega)$ are continuous.
\end{example*}

\begin{definition*}
Two S.P. $\{X_t\}$ and $\{Y_t\}$ are called \emph{versions} of one another if they have the same f.d.d.s.
\end{definition*}

\begin{definition*}
A S.P. $\{Y_t\}$ is called a \emph{modification} of another S.P. $\{X_t\}$ if $\mathbf P(X_t = Y_t) = 1$ for all $t$.
\end{definition*}

\begin{exercise*}
If $\{Y_t\}$ is a modification of $\{X_t\}$ then $\{Y_t\}$ is also a version of $\{X_t\}$.
\end{exercise*}

\begin{example*}
Consider $\Omega = \{HH,TT,HT,TH\}$ with uniform probability measure, corresponding to two indepdent fair coin tosses with outcome $\omega=(\omega_1,\omega_2)$. Let $X_t(\omega) = \1_{[0,1)}(t)I_H(\omega_1) + \1_{[1,2)}(t)I_H(\omega_2)$ for $0\leq t<2$, and let $Y_t(\omega) = 1-X_t(\omega)$. These are version of each other but not modifications of each other.
\end{example*}

\begin{definition*}
We say that a collection of f.d.d.s is \emph{consistent} if 
\[
	\lim_{\alpha_k \uparrow \infty} F_{t_1,\dots,t_N}(\alpha_1,\dots,\alpha_N) = F_{t_1,\dots,t_{k-1},t_{k+1},\dots,t_N} (\alpha_1,\dots,\alpha_{k-1},\alpha_{k+1},\dots,\alpha_N),
\]
for any $1\leq k\leq N$, $t_1<t_2<\dots<t_N$ and $\alpha_i\in\mathbb R$.
\end{definition*}

\begin{proposition*}
The f.d.d.s of any S.P. must be consistent. Conversely, for any consistent collection f.d.d.s, there exists a probability space $(\Omega, \F,\mathbf P)$ and a stochastic process $\{X_t(\omega)\}$ on it, whose f.d.d.s are in agreement with the given collection. Further, the restriction of $\mathbf P$ to the $\sigma$-field $\F_X$ is uniquely determined by the given collection of f.d.d.
\end{proposition*}

%excluding the rest of 3.1 because don't have to know about cylindrical \sigma-field stuff

\subsection*{3.2: Characteristic functions, Gaussian variables and processes}
\begin{definition*}
A \emph{random vector} $\uX = (X_1,\dots,X_n)$ with values in $\mathbb R^n$ has the \emph{characteristic function} 
$\Phi_{\uX}(\utheta) = \E[\exp{(i \sum_{k=1}^n \theta_k X_k)}]$, 
where $\utheta = (\theta_1,\dots,\theta_n) \in \mathbb R^n$.
\end{definition*}

\begin{remark*}
The characteristic function exists for any $\uX$ because trig functions are bounded.
\end{remark*}

\begin{proposition*}
The characteristic function determines the law of a random vector. That is, if $\Phi_\uX(\utheta) = \Phi_{\underline{Y}}(\utheta)$ for all $\utheta$ then $\uX$ has the same law (= probability measure on $\mathbb R^n$) as $\underline Y$.
\end{proposition*}

\begin{exercise*}
If $X_n \overset{\mathcal L}{\to} X$ then $\Phi_{X_n}(\theta) \to \Phi_X(\theta)$ for any $\theta$.
\end{exercise*}

\begin{remark*}
The converse of the previous exercise is also true.
\end{remark*}

\begin{definition*}
We say that a random vector $\uX = (X_1,\dots,X_n)$ has a \emph{probability density function} $f_\uX$ if 
\[
	\mathbf P(a_i \leq X_i \leq b_i,i=1,\dots,n) = \int_{a_1}^{b_1} \cdots \int_{a_n}^{b_n} f_\uX(x_1,\dots,x_n)\,dx_n\,\cdots\,dx_1
\]
for every $a_i<b_i,i=1,\dots,n$. Such density $f_\uX$ must be a non-negative Borel measurable function with $\int_{\mathbb R^n} f_\uX(\underline x) \, d\underline x = 1$. $f_\uX$ can be called the \emph{joint density} of $X_i$.
\end{definition*}

\begin{proposition*}
If $\uX = (X_1,\dots,X_n)$, with $X_i$ R.V.s, then $X_i$ are mutually independent if and only if $\Phi_\uX(\utheta) = \E \left[ \prod_{k=1}^n e^{i\theta_kX_k}\right] = \prod_{k=1}^n \Phi_{X_k}(\theta_k)$ for all $\utheta \in \mathbb R^n$.
\end{proposition*}

%skipping the note about positive semidefinite

\begin{definition*}
We say that a \emph{random vector} $\uX=(X_1,\dots,X_n)$ has a \emph{Gaussian} (or multivariate Normal) distribution if $\Phi_\uX(\utheta) = \exp{(-\frac12 \langle \utheta, \Sigma\utheta \rangle + i \langle \utheta, \underline \mu \rangle)}$ for some positive semidefinite $n\times n$ matrix $\Sigma$, some $\underline \mu \in \mathbb R^n$, and all $\utheta \in \mathbb R^n$.
\end{definition*}

\begin{remark*}
When $n=1$ we say that a R.V. $X$ is Gaussian if $\E[e^{i\theta X}] = e^{-\frac12 \theta^2 \sigma^2 + i\theta\mu}$.
\end{remark*}

\begin{definition*}
We say that $\uX$ has a \emph{non-degenerate} Gaussian distribution if $\Sigma$ is invertible.
\end{definition*}

\begin{proposition*}
A random vector $\uX$ with a non-degenerate Gaussian distribution has the density
$f_\uX(\underline x) = (2\pi)^{-n/2}(\det\Sigma)^{-1/2} \exp{(-\frac12 \langle\underline x - \underline \mu, \Sigma^{-1}(\underline x - \underline \mu)\rangle)}$. In particular, if $\sigma^2 > 0$, then a Gaussian R.V. $X$ has density $f_X(x) = (2\pi)^{-1/2} \sigma^{-1} \exp{(-\frac12 (x-\mu)^2/\sigma^2)}$.
\end{proposition*}

\begin{proposition*}
The \emph{parameters of the Gaussian distribution} are $\mu_j = \E X_j$ and $\Sigma_{jk} = \E[(X_j-\mu_j)(X_k-\mu_k)]$.
\end{proposition*}

% skipped the exercises but they might be important?

\begin{proposition*}
If a Gaussian random vector $\uX = (X_1,\dots,X_n)$ has uncorrelated coordinates, then its coordinates are also mutually independent.
\end{proposition*}

\begin{proposition*}
Suppose a sequence of $n$-dimensional Gaussian random vectors $\uX^{(k)}$ converges in 2-mean to an $n$-dimensional random vector $\uX$, that is, $\E[(X_i - X_i^{(k)})^2] \to 0$ as $k\to\infty$ for each $i$. Then, $\uX$ is a Gaussian random vector, whose parameters are the limits of the corresponding parameters of $\uX^{(k)}$.
\end{proposition*}

\begin{proposition*}
A random vector $\uX$ has the Gaussian distribution if and only if $(\sum_{i=1}^n a_{ji}X_i, j=1,\dots,m)$ is a Gaussian random vector for any non-random coefficients $a_{ij} \in \mathbb R$.
\end{proposition*}

\begin{definition*}
A stochastic process (S.P.) $\{X_t\}$ is \emph{Gaussian} if for all $n < \infty$ and all $t_1,\dots,t_n \in \mathcal I$ the random vector $(X_{t_1},\dots,X_{t_n})$ has a Gaussian distribution, that is, all f.d.d.s of the process are Gaussian
\end{definition*}

\begin{corollary*}
All distributional properties of Gaussian processes are determined by the mean $\mu(t) = \E X_t$ of the process and its auto-covariance function $\rho(t,s) = \E[(X_t-\mu(t))(X_s-\mu(s))]$.
\end{corollary*}

\subsection*{3.3: Sample path continuity}

