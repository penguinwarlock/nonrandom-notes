\section*{Chapter 1. Probability, measure, and integration}
\subsection*{1.1. Probability spaces and $\sigma$-fields}
\begin{definition*}[$\sigma$-field]
We say that $\F \subset 2^\Omega$ is a $\sigma$-field (or a $\sigma$-algebra), if
\begin{enumerate}[label=(\alph*)]
\item $\Omega \in \F$,
\item If $A \in \F$, then $A^c \in \F$ as well,
\item If $A_i \in \F$ for $i = 1, 2, \dots$, then also $\cup_{i=1}^\infty A_i
	\in \F$.
\end{enumerate}
\end{definition*} 

\begin{remark*} 
Using DeMorgan's law, you can easily check that if $A_ \in \F$ for $i = 1, 2,
\dots$ and $\F$ is a $\sigma$-field, then also $\cap_i A_i \in \F$. Similarly,
you can show that a $\sigma$-field is closed under countable many elementary
operations.
\end{remark*} 

\begin{definition*}
A pair $(\Omega, \F)$ with $\F$ a $\sigma$-field of subsets of $\Omega$ is
called a \emph{measurable space}. Given a measurable space, a \emph{probability
measure} $\Pb$ is a function $\Pb: \F \rightarrow [0, 1]$, having the following
properties:
\begin{enumerate}[label=(\alph*)]
\item $0 \le \Pb(A) \le 1$ for all $A \in \F$,
\item $\Pb(\Omega) = 1$,
\item (Countable additivity) $\Pb(A) = \sum_{n=1}^\infty \Pb(A_n)$ whenever $A =
\cup_{n=1}^\infty A_n$ is a countable union of disjoint sets $A_n \in \F$.
\end{enumerate}
A \emph{probability space} is a triplet $(\Omega, \F, \Pb)$ with $\Pb$ a
probability measure on the measurable space $(\Omega, \F)$.
\end{definition*} 

\begin{definition*}
Given a collection of subsets $A_\alpha \subseteq \Omega$, where $\alpha \in
\Gamma$ is a not necessarily countable index set, we denote the smallest
$\sigma$-field $\F$ such that $A_\alpha \in \F$ for all $\alpha \in \Gamma$ by
$\sigma(\{A_\alpha\})$ (or sometimes by $\sigma(A_\alpha, \alpha \in \Gamma)$,
and call $\sigma(\{A_\alpha\})$ the $\sigma$-field \emph{generated} by the collection
$\{A_\alpha\}$. That is
$\sigma(\{A_\alpha\}) - \cap \{ \G : \G \subset 2^\Omega
\text{ is a $\sigma$-field}, A_\alpha \in \G \forall \alpha \in
\Gamma\}$.
\end{definition*} 

\begin{example*} 
An example of a generated $\sigma$-field is the \emph{Borel $\sigma$-field} on
$\R$. It may be defined as $\B = \sigma(\{(a, b) : a, b \in \R\})$.
\$
\end{example*} 

\begin{lemma*} 
If two different collections of generators $\{A_\alpha\}$ and $\{B_\beta\}$ are
such that $A_\alpha \in \sigma(\{B_\beta\})$ for each $\alpha$ and $B_\beta \in
\sigma(\{A_\alpha\})$ for each $\beta$, then $\sigma(\{A_\alpha\}) =
\sigma(\{B_\beta\})$.
\end{lemma*} 

\begin{proposition*} 
There exists a subset of $\R$ that is not in $\B$. That is, not all sets are
Borel sets.
\end{proposition*} 


\subsection*{1.2: Random variables and their expectation}
\begin{definition*}
A \emph{Random Variable} (R.V.) is a function $X: \Omega \rightarrow \R$ such
that $\forall \alpha \in \R$ the set $\{\omega: X(\omega) \le \alpha\}$ is in
$\F$ (such a function is also called a \emph{$\F$-measurable or, simply,
measurable function}).
\end{definition*} 

%Omitted indicator function def. I think we know it.

\begin{remark*} 
Note that $X(\omega) = \sum_{n=1}^N c_n I_{A_n} (\omega)$ is a R.V. for any
finite $N$, non-random $c_n \in \R$ and sets $A_n \in \F$. We call any such $X$
a simple function, denoted $X \in \SF$.
\end{remark*} 

\begin{proposition*} 
For every R.V. $X(\omega)$ there exists a sequence of simple functions
$X_n(\omega)$ such that $X_n(\omega) \rightarrow X(\omega)$ as $n\rightarrow
\infty$, for each fixed $\omega \in \Omega$.
\end{proposition*} 

\begin{definition*}
We say that a R.V. $X$ and $Y$ are defined on the same probability space
$(\Omega, \F, \Pb)$ are almost surely the same if $\Pb(\{\omega: X(\omega) \ne
Y(\omega)\}) = 0$. This shall be denoted by $X \as Y$. We also use the terms
\emph{almost surely}, \emph{almost everywhere,} and \emph{with probability 1
interchangeably}.
\end{definition*} 

\begin{definition*}
Given a R.V. $X$ we denote $\sigma(X)$ the smallest $\sigma$-field $\G \subseteq
\F$ such that $X(\omega)$ is measure on $(\Omega, \G)$. One can show that
$\sigma(X) = \sigma(\{\omega: X(\omega) \le \alpha\})$. We call $\sigma(X)$ the
$\sigma$-field generated by $X$ and interchangeably use the notations
$\sigma(X)$ and $\F_X$. Similarly, given R.V. $X_1, \dots, X_n$ on the same
measurable space $(\Omega, \F)$, denote $\sigma(X_k, k \le n)$ the smallest
$\sigma$-field $\F$ such that all $X_k(\omega)$ are measurable on $(\Omega,
\F)$.
\end{definition*} 

\begin{exercise*} 
Let $(\Omega, \F)$ be a measurable space and let $X_n$ be a sequence of random
variables on it. Assume that for each $\omega \in \Omega$, the limit $X_\infty
(\omega) = \lim_{n\rightarrow \infty} X_n(\omega)$ exists and is finite. Prove
that $X_\infty$ is a random variable on $(\Omega, \F)$.
\end{exercise*} 

\begin{definition*} 
A function $g: \R \rightarrow \R$ is called \emph{Borel (measurable) function}
if $g$ is a R.V. on $(\R, \B)$. if $g$ is a R.V. on $(\R, \B)$. We shall extend
the notion of Borel sets and functions to $\R^n$ by defining the Borel
$\sigma$-field on $\R^n$ as $\B_n = \sigma(\{[a_1, b_1] \times \cdots \times
[a_n, b_n] : a_i, b_i \in \R, i = 1, \dots, n\})$ and calling $g: \R^n
\rightarrow \R$ a Borel function if $g$ is a R.V. on $(\R^n, \B_n)$.
\end{definition*} 

\begin{proposition*} 
If $g: \R^n \rightarrow \R$ is a Borel function and $X_1, \dots, X_n$ are R.V.
on $(\Omega, \F)$, then $g(X_1, \dots, X_n)$ is also a R.V. on $(\Omega, \F)$.
\end{proposition*} 

\begin{theorem*} 
If $Z$ is a R.V. on $(\Omega, \sigma(Y_1, \dots, Y_n))$, then $Z = g(Y_1, \dots,
Y_n)$ for some Borel function $g: \R^n \rightarrow \R$.
\end{theorem*} 

\begin{proposition*} 
For any $n < \infty$, any Borel function $g: \R^n \rightarrow \R$ and R.V. $Y_1,
\dots, Y_n$ on the same measurable space we have the inclusion
$\sigma(g(Y_1, \dots, Y_n)) \subseteq \sigma(Y_1, \dots, Y_n)$.
\end{proposition*} 

\begin{corollary*} 
Suppose R.V. $Y_1, \dots, Y_n$ and $Z_1, \dots, Z_m$ defined on the same
measurable space are such that $Z_k = g_k(Y_1, \dots, Y_n)$, $k = 1, \dots, m$
and $Y_i = h_i(Z_1, \dots, Z_m)$, $i = 1, \dots, n$ for some Borel functions
$g_k: \R^n \rightarrow \R$ and $h_i: \R^m \rightarrow \R$. Then $\sigma(Y_1,
\dots, Y_n) = \sigma(Z_1, \dots, Z_m)$.
\end{corollary*} 

\begin{definition*} 
The (mathematical) \emph{expectation} of a R.V. $X(\omega)$ is denote $\E X$.
With $x_{k, n} = k2^{-n}$ and the intervals $I_{k, n} = (x_{k, n}, x_{k+1, n}]$
for $k = 0, 1, \dots$, the expectation of $X(\omega) \ge 0$ is defined as:
$\E X = \lim_{n\rightarrow \infty} \bracket{
	\sum_{k=0}^\infty x_{k, n} \Pb(\{\omega: X(\omega) \in I_{k, n} \})}$.
\end{definition*} 

\begin{example*} 
Though note detailed in these notes, it is possible to show that for $\Omega$
countable and $\F = 2^\Omega$ our definition coincides with the well know
elementary definition $\E X = \sum_\omega X(\omega) p_\omega$ (where $X(\omega)
\ge 0)$. More generally, the formula $\E X = \sum_i x_i \Pb(\{\omega :
X(\omega) = x_i\})$ applies whenever the range of $X$ is a bounded below countable
set $\{x_1, x_2, \dots\}$ of real numbers e.g. whenever $X \in \SF$.
\end{example*} 

\begin{remark*} 
Using the elementary formula $\E Y = \sum_{m=1}^N c_m \Pb(A_m)$ for the simple
function $Y(\omega) = \sum_{m=1}^N c_m I_{A_m} (\omega)$, it can be shown that
our definition of the expectation of $X \ge 0$ coincides with $\E X = \sup\{ \E
Y: Y \in \SF, 0 \le Y \le X\}$.
\end{remark*} 

\begin{definition*} 
We say that a R.V. $X(\omega)$ has \emph{a probability density function} $f_X$
if $\Pb(a \le X \le b) = \int_a^b f_X(x)\,dx$ for every $a < b \in \R$. Such
$f_X$ must be a non-negative function with $\int_{\R} f_X(x)\,dx = 1$.
\end{definition*} 

\begin{proposition*} 
When a non-negative R.V. $X(\omega)$ has a probability density function $f_X$,
our definition of the expectation coincides with the well known elementary
formula $\E X = \int_0^\infty x f_X(x)\,dx$.
\end{proposition*} 

\begin{remark*} 
%(Not in Remark in notes. Also paraphrased.)
The first definition of expectation is also called the \emph{Lebesgue integral}
of $X$ with respect to the probability measure $\Pb$ and consequently denoted
$\E X = \int X(\omega) d\Pb(\omega)$ (or $\int X(\omega) \Pb(d\omega)$). It is
based on splitting the \emph{range} of $X(\omega)$ to finite many small
intervals. This allows us to deal with rather general domain $\Omega$, in
contrast to Riemann's integral where the \emph{domain} of integration is split
into finitely many small intervals and is hence limited to $\R^d$. Even when
$\Omega = [0, 1]$ it allows us to deal with measures $\Pb$ for which $\omega
\rightarrow \Pb([0, \omega])$ is not smooth (and hence Riemann's integral fails
to exist). If the corresponding Riemann integral exists, then it necessarily
coincides with our Lebesgue integral.
\end{remark*} 

\begin{definition*} 
For a general R.V. $X$ consider the non-negative R.V.-s $X_+ = max(X, 0$ and
$X_{-} = -\min(X, 0)$, so $X = X_+ - X_-$. Let $\E X = \E X_+ - \E X_-$,
provided either $\E X_+ < \infty$ or $\E X_- < \infty$.
\end{definition*} 

\begin{definition*} 
We say that a random variable $X$ is \emph{integrable} (or has finite
expectation) if $\E|X| < \infty$, that is, both $\E X_+ < \infty$ and $\E X_- <
\infty$.
\end{definition*} 

\begin{remark*} 
Suppose $X = Y - Z$ for some non-negative R.V.-s $Y$ and $Z$. Then, necessarily
$Y = D + X_+$ and $Z = D + X_-$ for some $D \ge 0$ and all $\omega \in \Omega$.
It can be shown that the expectation is linear with respect to the addition of
non-negative R.V. In particular, $\E Y = \E D + \E X_+$ and $\E Z = \E D + \E
X_-$. Therefore, $\E Y - \E Z = \E X_+ - \E X_- = \E X$, provided either $\E Y$ or
$\E Z$ is finite. We conclude in this case that $\E X = \E Y - \E Z$. However,
it is possible to have $X$ integrable while $\E D = \infty$ resulting with $\E Y
= \E Z = \infty$.
\end{remark*} 

\begin{proposition*} 
If a R.V. $X$ has the probability density function $f_X$ and $h: \R \rightarrow
\R$ is a Borel measurable function, then the R.V. $Y = h(X)$ is integrable if
and only if $\int_{-\infty}^\infty |h(x)| f_X (x)\,dx < \infty$, in which case
$\E Y = \int_{-\infty}^\infty h(x) f_X (x)\,dx$.
\end{proposition*} 

\begin{definition*} 
A R.V. $X$ with probability density function $f_X(x) = \frac{1}{\sqrt{2\pi}
\sigma} \exp\paren{ - \frac{(x - \mu)^2}{2\sigma^2}}$ for $x \in \R$ where $\mu
\in \R$ and $\sigma > 0$ is called a non-degenerate Gaussian (or Normal) R.V.
with mean $\mu$ and variance $\sigma^2$, denoted by $X \sim N(\mu, \sigma^2)$.
\end{definition*} 

\begin{proposition*} 
The expectation has the following properties.
\begin{enumerate}
\item $\E I_A = \Pb(A)$ for any $A \in \F$.
\item If $X(\omega) = \sum_{n=1}^N c_n I_{A_n}$ is a simple function, then
$\E X = \sum_{n=1}^N c_n \Pb(A_n)$.
\item If $X$ and $Y$ are integrable R.V. then for any constants $\alpha, \beta$
the R.V. $\alpha X + \beta Y$ is integrable and $\E(\alpha X + \beta Y) = \alpha
\E X + \beta \E Y$.
\item $\E X = c$ if $X(\omega) = c$ with probability 1.
\item Monotonicity: If $X \ge Y$ a.s., then $\E X \ge \E Y$. Further, if $X \ge
Y$ a.s. and $\E X = \E Y$, then $X = Y$ a.s.
\end{enumerate}
\end{proposition*} 

\begin{proposition*}[Jensen's inequality]
Suppose $g(\cdot)$ is a convex function. If $X$ is an integrable R.V. and $g(X)$
is also integrable, then $\E(g(x)) \ge g(\E X)$.
\end{proposition*} 

\begin{theorem*}[Markov's inequality]
Suppose $f$ is a non-decreasing, Borel measurable function with $f(x) > 0$ for
any $x > 0$. Then, for any random variable $X$ and all $\epsilon > 0$,
$\Pb(|X(\omega)| > \epsilon) \le \frac{1}{f(\epsilon)} \E(f(|X|))$.
\end{theorem*} 

\begin{proposition*} 
Suppose $Y$ and $Z$ are random variables on the same probability space with both
$\E[Y^2]$ and $\E[Z^2]$ finite. Then, $\E|YZ| \le \sqrt{\E Y^2 \E Z^2}$.
\end{proposition*} 

\subsection*{1.3: Convergence of random variables}
\begin{definition*} 
We say that random variables $X_n$ converge to $X$ almost surely, denoted
$X_n \xrightarrow{a.s.} X$, if there exists $A \in \F$ with $\Pb(A) = 1$ such
that $X_n(\omega) \rightarrow X(\omega)$ as $n \rightarrow \infty$ for each
fixed $\omega \in A$.
\end{definition*} 

\begin{exercise*}
Show that if $X_n \xrightarrow{a.s.} X$ and $f$ is a continuous function then
$f(X_n) \xrightarrow{a.s.} f(X)$ as well.
\end{exercise*}

\begin{definition*} 
We say that $(\Omega, \F, \Pb)$ is a \emph{complete probability space} if any
subset $N$ of $B \in \F$ with $\Pb(B)$ is also in $\F$.
\end{definition*} 

\begin{definition*} 
We say that $X_n$ converges to $X$ in probability, denoted $X_n \rightarrow_p
X$, if $\Pb(\{\omega: |X_n(\omega) - X(\omega)| > \epsilon\})$ as $n\rightarrow
\infty$, for any fixed $\epsilon > 0$.
\end{definition*} 

\begin{theorem*} 
We have the following relations:
\begin{itemize}
\item If $X_n \xrightarrow{a.s} X$, then $X_n \rightarrow_p X$.
\item If $X_n \rightarrow_p X$, then there exists a subsequence $n_k$ such that
$X_{n_k} \xrightarrow{a.s.} X$ for $k \rightarrow \infty$.
\end{itemize}
\end{theorem*} 

\begin{proposition*} 
In general, $X_n \rightarrow_p X$ does not imply $X_n \xrightarrow{a.s.} X$.
\end{proposition*} 

\begin{definition*} 
Let $\{A_n\}$ be a sequence of events, and $B_n = \cup_{k=n}^\infty A_k$. Define
$A^\infty = \cap_{n=1}^\infty B_n$, so $\omega \in A^\infty$ if and only if
$\omega \in A_k$ for infinitely many values of $k$.
\end{definition*} 

\begin{lemma*}[Borel-Cantelli I]
Suppose $A_k \in \F$ and $\sum_{k=1}^\infty \Pb(A_k) < \infty$. Then,
necessarily $\Pb(A^\infty) = 0$.
\end{lemma*} 

\begin{lemma*}[Borel-Cantelli II]
If $\{A_k\}$ are independent and $\sum_{k=1}^\infty \Pb(A_k) = \infty$, then
$\Pb(A^\infty) = 1$.
\end{lemma*} 

\begin{proposition*} 
Suppose $\E[X_n^2] \le 1$ for all $n$. Then $n^{-1} X_n(\omega) \rightarrow 0$
a.s. for $n\rightarrow \infty$.
\end{proposition*} 

\begin{definition*} 
Fixing $1 \le q < \infty$, we denote $L^q(\Omega, \F, \Pb)$ the collection of
random variables $X$ on $(\Omega, \F)$ for which $\E(|X|^q) < \infty$.
$L^1$ denotes the space of integrable functions, and $L^2$ denotes the space of
square-integrable functions.
\end{definition*}

\begin{proposition*} 
The sequence $||X||_q = [\E(|X|^q)]^{1/q}$ is non-decreasing in $q$.
\end{proposition*} 

\begin{definition*} 
We say that $X_n$ converges in $q$-mean, or in $L^q$ to $X$, denoted
$X_n \xrightarrow{q.m.} X$ if $X_n, X \in L^q$ and $||X_n - X||_q \rightarrow 0$
as $n\rightarrow \infty$ (i.e. $\E(|X_n - X|^q) \rightarrow 0$) as $n\rightarrow
\infty$.
\end{definition*} 

\begin{corollary*} 
If $X_n \xrightarrow{q.m.} X$ and $q \ge r$, then $X_n \xrightarrow{r.m.} X$.
\end{corollary*} 

\begin{proposition*} 
$L^q(\Omega, \F, \Pb)$ is a complete, normed (topological) vector space with the
norm $||\cdot||_q$. That is $\alpha X + \beta Y \in L^q$ whenever $X, Y \in
L^q$, $\alpha, \beta \in \R$, with $X \mapsto ||X||_q$ a norm on $L^q$ and if $X_n
\in L^q$ and if $X_n \in L^q$ are such that $||X_n - X_m|| \rightarrow 0$ as
$n, m \rightarrow \infty$ then $X_n \xrightarrow{q.m.} X$ for some $X \in L^q$.
\end{proposition*}

\begin{proposition*} 
If $X_n \xrightarrow{q.m.} X$, then $X_n \rightarrow_p X$.
\end{proposition*} 

\begin{example*} 
The converse of the above does not hold in general.
%Long example?
\end{example*} 

\begin{exercise*}
Give a \textbf{counterexample} to each of the following claims:
\begin{itemize}
\item If $X_n \rightarrow X$ a.s., then $X_n \rightarrow X$ in $L^q$, $q \ge 1$;
\item If $X_n \rightarrow X$ in $L^q$ then $X_n \rightarrow X$ a.s.;
\item If $X_n \rightarrow X$ in probability then $X_n \rightarrow X$ a.s.
\end{itemize}
\end{exercise*}

\begin{proposition*} 
If $X_n \xrightarrow{q.m.} X$ and $X_n \xrightarrow{a.s.} Y$, then $X = Y$ a.s.
\end{proposition*} 

\subsection*{1.4: Independence, weak convergence and uniform integrability}

\begin{definition*} 
The law of a R.V. $X$, denoted $\law_X$, is the probability measure on $(\R,
\B)$ such that $\law_X(B) = \Pb(\{\omega: X(\omega) \in B\})$ for any Borel set
$B$.
\end{definition*} 

\begin{proposition*} 
Let $X$ be a R.V. on $(\Omega, \F, \Pb)$ and let $g$ be a Borel function of
$\R$. Suppose either $g$ is non-negative or $\E|g(X)| < \infty$. Then,
$\E[g(X)] = \int_\R g(x) d\law_X(x)$, where the integral on the rhs merely
denotes the expectation of the random variables $g(x)$ on the (new) probability
space $(\R, \B, \law_X)$.
\end{proposition*} 

\begin{definition*} 
The distribution function $F_X$ of a real-valued R.V. $X$ is
$F_X(\alpha) = \Pb(\{\omega: X(\omega) < \alpha\}) = \law_X((-\infty, \alpha])$
for all $\alpha \in \R$.
\end{definition*} 

\begin{proposition*} 
The distribution function $F_X$ uniquely determines the law $\law_X$ of $X$.
\end{proposition*} 

\begin{proposition*} 
A R.V. $X$ has a (probability) density (function) $f_X$ if and only if its
distribution function $F_X$ can be expressed as $F_X(\alpha) =
\int_{-\infty}^\alpha f_X(x)\,dx$, for all $\alpha \in \R$. Such $F_X$ is
continuous and almost everywhere differentiable with $\frac{dF_X}{dx}(x) =
f_X(x)$ for almost every $x$.
\end{proposition*}

\begin{definition*} 
We say that R.V.-s $X_n$ converge in law (or weakly) to a R.V. $X$, denoted by
$X_n \xrightarrow{\mL} X$ if $F_{X_n} (\alpha) \rightarrow F_X (\alpha)$ as $n
\rightarrow \infty$ for each fixed $\alpha$ which is a continuity point of
$F_X$. In other books, this is called convergence in distribution, and denoted
$X_n \xrightarrow{\mathcal{D}} X$.
\end{definition*} 

\begin{proposition*} 
$X_n \xrightarrow{\mL} X$ if and only if for each bounded $h$ that is continuous
on the range of $X$ we have that $\E h(X_n) \rightarrow \E h(X)$ as $n
\rightarrow \infty$.
\end{proposition*} 

\begin{remark*} 
Note that $F_X(\alpha) = \law_X ((-\infty, \alpha]) = \E[I_{(-\infty, \alpha]}
(X)]$ involves the function $h(x) = 1_{(-\infty, \alpha]}(x)$. Restricting the
convergence in law to continuity points of $F_X$ is what makes the above
possible.
\end{remark*} 

\begin{proposition*} 
If $X_n \rightarrow_p X$, then $X_n \xrightarrow{\mL} X$.
\end{proposition*} 

\begin{proposition*} 
If $X_n \xrightarrow{\mL} X$ and $X$ is a non-random constant (almost surely),
then $X_n \rightarrow_p X$ as well.
\end{proposition*} 

\begin{definition*} 
We say that a sequence of probability measure $Q_n$ on a topological space $\bS$
(i.e. a set with a notion of open sets, or topology) and its Borel
$\sigma$-field $\B_\bS$ ($ = $ the $\sigma$-field generated by the open subsets
of $\bS$), converges weakly to a probability measure $Q$ if for each fixed $g$
continuous and bounded on $\bS$, $\int_\bS h(\omega) dQ_n(\omega) \rightarrow
\int_\bS h(\omega) dQ(\omega)$ as $n \rightarrow \infty$ (the integrals denote
the expectation of the R.V. $h(\omega)$ in their respective probability spaces).
We shall use $Q_n \Rightarrow Q$ to denote weak convergence.
\end{definition*} 

\begin{example*} 
$X_n \xrightarrow{\mL} X$ if and only if $\law_{X_n} \Rightarrow \law_X$. That
is, when $\int_\R h(\xi) d\law_{X_n} (\xi) \rightarrow \int_\R h(\xi)
d\law_X(\xi)$, for each fixed $h: \R \rightarrow \R$ continuous and bounded.
\end{example*} 

\begin{theorem*}[Dominated Convergence]
If there exists a random variable $Y$ such that $\E Y < \infty$, $|X_n| \le Y$
for all $n$ and $X_n \rightarrow_p X$, then $\E X_n \rightarrow \E X$.
\end{theorem*} 

\begin{corollary*}[Bounded Convergence]
Suppose $|X_n| \le C$ for some finite constance $C$ and all $n$. If $X_n
\rightarrow X$, then also $\E X_n \rightarrow \E X$.
\end{corollary*} 

\begin{theorem*}[Monotone Convergence]
If $X_n \ge 0$ and $X_n(\omega)\uparrow X(\omega)$ for a.e. $\omega$, then
$\E X_n \uparrow \E X$. This applies even if $X(\omega) = \infty$ for some
$\omega \in \Omega$.
\end{theorem*} 

\begin{definition*} 
Events $A_i \i \F$ are $\Pb$-mutually independent if for any $L < \infty$ and
distinct indices $i_1, i_2, \dots, i_L$,
$\Pb(A_{i_1} \cap A_{i_2} \cap \dots \cap A_{i_L}) = \prod_{k=1}^L
\Pb(A_{i_k})$.
\end{definition*} 

\begin{definition*} 
Two $\sigma$-fields $\Hi, \G \subseteq \F$ are $\Pb$-independent if $\Pb(G \cap
H) = \Pb(G) \Pb(H)$ for all $G \in \G, H \in \Hi$. The random vectors vectors
$(X_1, \dots, X_n)$ and $(Y_1, \dots, Y_m)$ are independent if their
corresponding generated $\sigma$-fields are independent.
\end{definition*} 

\begin{proposition*} 
For any finite $n, m \ge 1$, two random vector $(X_1, \dots, X_n)$ and $(Y_1,
\dots, Y_m)$ with values in $\R^n$ and $\R^m$, respectively, are independent if
and only if $\E(h(X_1, \dots, X_n) g(Y_1, \dots, Y_m))
= \E(h(X_1, \dots, X_n)) \E(g(Y_1, \dots, Y_m))$, for all bounded, Borel
measurable function $g: \R^m \rightarrow \R$ and $h: \R^n \rightarrow \R$.
\end{proposition*} 

\begin{definition*} 
Square-integrable random variables $X$ and $Y$ defined on the same probability
space are called uncorrelated if $\E(XY) = \E(X) \E(Y)$.
\end{definition*} 

\begin{remark*} 
Independent random variables are uncorrelated, but the converse is not
neccessarily true.
\end{remark*} 

\begin{proposition*} 
Any two square-integrable independent randome variables $X$ and $Y$ are also
uncorrelated.
\end{proposition*} 







